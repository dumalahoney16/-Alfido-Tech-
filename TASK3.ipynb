{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaktHDfW0Tp7",
        "outputId": "08d1eb65-00c1-41fd-f5e8-004c483127e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import FreqDist\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify import accuracy as nltk_accuracy\n",
        "import random\n"
      ],
      "metadata": {
        "id": "0aX4egeY1QXL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbZFPRbZ1SAf",
        "outputId": "79400334-0327-45a5-e820-cb8fe8e96c6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "print(\"Total Documents:\", len(documents))\n",
        "print(\"Sample Document Words:\", documents[0][0][:20])\n",
        "print(\"Category:\", documents[0][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jTKwpV1T-3",
        "outputId": "260aec30-56f4-4206-bc55-4600b08f68bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents: 2000\n",
            "Sample Document Words: ['of', 'circumcision', ',', 'psychic', 'wounds', 'and', 'the', 'family', 'sitcom', 'the', 'opening', 'segment', 'is', 'something', 'of', 'a', 'foretaste', 'of', 'this', 'film']\n",
            "Category: pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = FreqDist(w.lower() for w in movie_reviews.words())\n",
        "\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "def find_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "    return features\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "\n",
        "print(\"Total Feature Sets:\", len(featuresets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCILTOKC1XrX",
        "outputId": "9ecdfece-6d25-4b9e-a9b9-3fcab83f1432"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Feature Sets: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(featuresets) * 0.8)\n",
        "train_set = featuresets[:train_size]\n",
        "test_set = featuresets[train_size:]\n",
        "\n",
        "print(\"Training samples:\", len(train_set))\n",
        "print(\"Testing samples:\", len(test_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7xdvhZ81el-",
        "outputId": "90b652be-3b20-4a4b-ba04-26fcfb53db3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1600\n",
            "Testing samples: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk_accuracy(classifier, test_set)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVKI2XOt1iAh",
        "outputId": "ae5f29c2-ca6c-4770-8bde-09ab892c7a0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 82.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqE8B3N61mtJ",
        "outputId": "1266fcaa-ea14-4460-d909-dbd602ec2e95"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             outstanding = True              pos : neg    =     14.4 : 1.0\n",
            "                  seagal = True              neg : pos    =     13.0 : 1.0\n",
            "                   mulan = True              pos : neg    =      8.3 : 1.0\n",
            "             wonderfully = True              pos : neg    =      6.4 : 1.0\n",
            "               fantastic = True              pos : neg    =      6.2 : 1.0\n",
            "                  poorly = True              neg : pos    =      5.3 : 1.0\n",
            "                     era = True              pos : neg    =      5.1 : 1.0\n",
            "                   awful = True              neg : pos    =      5.1 : 1.0\n",
            "              ridiculous = True              neg : pos    =      5.0 : 1.0\n",
            "                    lame = True              neg : pos    =      5.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    words = word_tokenize(text)\n",
        "    features = find_features(words)\n",
        "    return classifier.classify(features)\n",
        "\n",
        "# Test examples\n",
        "print(\"\\nCustom Predictions:\")\n",
        "print(\"1. 'This movie was amazing and full of joy.' →\", predict_sentiment(\"This movie was amazing and full of joy.\"))\n",
        "print(\"2. 'The plot was boring and the acting was terrible.' →\", predict_sentiment(\"The plot was boring and the acting was terrible.\"))\n",
        "print(\"3. 'It was okay, not too bad but not great either.' →\", predict_sentiment(\"It was okay, not too bad but not great either.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkIDCJXP1oyD",
        "outputId": "8a330203-4b1c-4711-aab5-113666f90065"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Predictions:\n",
            "1. 'This movie was amazing and full of joy.' → neg\n",
            "2. 'The plot was boring and the acting was terrible.' → neg\n",
            "3. 'It was okay, not too bad but not great either.' → neg\n"
          ]
        }
      ]
    }
  ]
}